It was difficult understanding how to correctly parse though the html, and what dependencies were more useful than others. A majority of my code was based off online code, I simply warped it to do what I wanted it to do. In the end I was lucky that the webpages I was scrapping had similar outlines, and they had all the relevant information I needed in each div section. SO most of the data collecting was simply copying and pasting. But I'd rather have a script do that in 15sec for a few pages then the 20min it would take for a single page. 
One thing that I was not happy with my script, was the nested loops I had in the script, In the end I think it was O(N^3) since there were 3 nested loops each of varying length though, so it was some what efficient. The funniest part of this project was how fast it was to simply get over the hurdles, first open browser, second parse webpage, third transverse parsed data, find your specific data, repeat until needed data is reached. I added the panda data frame to see what analysis was possible with the overall data. The most challenging was definitely the searching of appropriate elements, since every webpage is a bit more unique then the last. But once a pattern was found it was simply a matter of understanding where it could get wrong data. Also getting rid of unneeded data after scrapping the webpages was annoying, more than finding the elements now that i think about it. The easy part was scraping the webpage, the hardest part was cleaning that data up to make it accurate and useful. 
Some advice I would give to a future student is understand what you want to do with the data before you go scrapping a bunch of webpages, also try to clean up data while collecting it and not after. To avoid a bunch of looping and debugging, trust me, once you have ugly or messy data, look at how you're collecting it. 
I would suggest to do data analysis before scripting or web scrapping specifically.	